#aplicando deployment padrão para testar taint e tolerations
kubectl apply -f ./taint_e_tolerations/deployment.yaml

#verificando execução do pod
kubectl get pods -o wide

NAME                               READY   STATUS    RESTARTS   AGE   IP            NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-7796b7557-7q855   1/1     Running   0          47s   100.64.2.52   scw-k8s-angry-lovelace-default-104ea93176284d6   <none>           <none>

#verificando nós do clustes
kubectl get nodes

NAME                                             STATUS   ROLES    AGE    VERSION
scw-k8s-angry-lovelace-default-104ea93176284d6   Ready    <none>   156m   v1.22.3
scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   Ready    <none>   156m   v1.22.3
scw-k8s-angry-lovelace-default-fe175434bbd547e   Ready    <none>   156m   v1.22.3

#verificando Taints atribuídos ao nó, deve estar com o valor <none>:
kubectl describe node scw-k8s-angry-lovelace-default-104ea93176284d6

Name:               scw-k8s-angry-lovelace-default-104ea93176284d6
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=DEV1-M
                    beta.kubernetes.io/os=linux
                    failure-domain.beta.kubernetes.io/region=fr-par
                    failure-domain.beta.kubernetes.io/zone=fr-par-1
                    k8s.scaleway.com/kapsule=39713063-f994-4dd6-8b91-f0d3fec3607a
                    k8s.scaleway.com/managed=true
                    k8s.scaleway.com/node=104ea931-7628-4d64-b901-85d3c55d499e
                    k8s.scaleway.com/pool=63f48a05-1ab1-49e1-916e-36aab1c39b88
                    k8s.scaleway.com/pool-name=default
                    k8s.scaleway.com/runtime=containerd
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=scw-k8s-angry-lovelace-default-104ea93176284d6
                    kubernetes.io/os=linux
                    node.kubernetes.io/instance-type=DEV1-M
                    topology.csi.scaleway.com/zone=fr-par-1
                    topology.kubernetes.io/region=fr-par
                    topology.kubernetes.io/zone=fr-par-1
Annotations:        csi.volume.kubernetes.io/nodeid: {"csi.scaleway.com":"fr-par-1/8e076646-d3f6-4531-915e-cdfd8b04dd28"}
                    io.cilium.network.ipv4-cilium-host: 100.64.2.130
                    io.cilium.network.ipv4-health-ip: 100.64.2.19
                    io.cilium.network.ipv4-pod-cidr: 100.64.2.0/24
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 02 Nov 2021 09:34:42 -0300
Taints:             <none>

#aplicando Taints de NoSchedule para que não permita novos agendamento de pods novos no nó scw-k8s-angry-lovelace-default-104ea93176284d6
kubectl taint nodes scw-k8s-angry-lovelace-default-104ea93176284d6 especial=valor1:NoSchedule

#verificando nós do clustes
kubectl describe nodes scw-k8s-angry-lovelace-default-104ea93176284d6

Name:               scw-k8s-angry-lovelace-default-104ea93176284d6
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=DEV1-M
                    beta.kubernetes.io/os=linux
                    failure-domain.beta.kubernetes.io/region=fr-par
                    failure-domain.beta.kubernetes.io/zone=fr-par-1
                    k8s.scaleway.com/kapsule=39713063-f994-4dd6-8b91-f0d3fec3607a
                    k8s.scaleway.com/managed=true
                    k8s.scaleway.com/node=104ea931-7628-4d64-b901-85d3c55d499e
                    k8s.scaleway.com/pool=63f48a05-1ab1-49e1-916e-36aab1c39b88
                    k8s.scaleway.com/pool-name=default
                    k8s.scaleway.com/runtime=containerd
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=scw-k8s-angry-lovelace-default-104ea93176284d6
                    kubernetes.io/os=linux
                    node.kubernetes.io/instance-type=DEV1-M
                    topology.csi.scaleway.com/zone=fr-par-1
                    topology.kubernetes.io/region=fr-par
                    topology.kubernetes.io/zone=fr-par-1
Annotations:        csi.volume.kubernetes.io/nodeid: {"csi.scaleway.com":"fr-par-1/8e076646-d3f6-4531-915e-cdfd8b04dd28"}
                    io.cilium.network.ipv4-cilium-host: 100.64.2.130
                    io.cilium.network.ipv4-health-ip: 100.64.2.19
                    io.cilium.network.ipv4-pod-cidr: 100.64.2.0/24
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 02 Nov 2021 09:34:42 -0300
Taints:             especial=valor1:NoSchedule
Unschedulable:      false


#escalando o deployment para verificar os nós que eles serão feitos deploy
kubectl scale deployment nginx-deployment --replicas=10

#verificando deployment executado
kubectl get pods -o wide

NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-7796b7557-589ph   1/1     Running   0          39s   100.64.0.218   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-7q855   1/1     Running   0          13m   100.64.2.52    scw-k8s-angry-lovelace-default-104ea93176284d6   <none>           <none>
nginx-deployment-7796b7557-944wt   1/1     Running   0          39s   100.64.1.186   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-bkv6r   1/1     Running   0          39s   100.64.0.91    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-c6jfp   1/1     Running   0          39s   100.64.0.105   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lm4nf   1/1     Running   0          40s   100.64.0.44    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lv5b7   1/1     Running   0          39s   100.64.1.30    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-t7gqg   1/1     Running   0          39s   100.64.1.79    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-xbzb7   1/1     Running   0          40s   100.64.1.219   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-z6lqx   1/1     Running   0          40s   100.64.1.165   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>

#aplicando Taints de NoExecute para que não permita mais execução de pod nenhum no nó scw-k8s-angry-lovelace-default-104ea93176284d6
kubectl taint nodes scw-k8s-angry-lovelace-default-104ea93176284d6 especial=valor1:NoExecute

#verificando deployment executado
kubectl get pods -o wide

NAME                               READY   STATUS    RESTARTS   AGE     IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-7796b7557-589ph   1/1     Running   0          8m38s   100.64.0.218   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-7gbsq   0/1     Pending   0          61s     <none>         <none>                                           <none>           <none>
nginx-deployment-7796b7557-944wt   1/1     Running   0          8m38s   100.64.1.186   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-bkv6r   1/1     Running   0          8m38s   100.64.0.91    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-c6jfp   1/1     Running   0          8m38s   100.64.0.105   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lm4nf   1/1     Running   0          8m39s   100.64.0.44    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lv5b7   1/1     Running   0          8m38s   100.64.1.30    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-t7gqg   1/1     Running   0          8m38s   100.64.1.79    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-xbzb7   1/1     Running   0          8m39s   100.64.1.219   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-z6lqx   1/1     Running   0          8m39s   100.64.1.165   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>

#ficou pendente por que não tem mais recurso de máquina para usar, mas foi removido o pod de execução (nginx-deployment-7796b7557-7q855)

#os Taints são "acumulados" ao ser adicionado no nó, podemos ver isso fazendo um describe no nó.
kubectl describe nodes scw-k8s-angry-lovelace-default-104ea93176284d6

Name:               scw-k8s-angry-lovelace-default-104ea93176284d6
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=DEV1-M
                    beta.kubernetes.io/os=linux
                    failure-domain.beta.kubernetes.io/region=fr-par
                    failure-domain.beta.kubernetes.io/zone=fr-par-1
                    k8s.scaleway.com/kapsule=39713063-f994-4dd6-8b91-f0d3fec3607a
                    k8s.scaleway.com/managed=true
                    k8s.scaleway.com/node=104ea931-7628-4d64-b901-85d3c55d499e
                    k8s.scaleway.com/pool=63f48a05-1ab1-49e1-916e-36aab1c39b88
                    k8s.scaleway.com/pool-name=default
                    k8s.scaleway.com/runtime=containerd
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=scw-k8s-angry-lovelace-default-104ea93176284d6
                    kubernetes.io/os=linux
                    node.kubernetes.io/instance-type=DEV1-M
                    topology.csi.scaleway.com/zone=fr-par-1
                    topology.kubernetes.io/region=fr-par
                    topology.kubernetes.io/zone=fr-par-1
Annotations:        csi.volume.kubernetes.io/nodeid: {"csi.scaleway.com":"fr-par-1/8e076646-d3f6-4531-915e-cdfd8b04dd28"}
                    io.cilium.network.ipv4-cilium-host: 100.64.2.130
                    io.cilium.network.ipv4-health-ip: 100.64.2.19
                    io.cilium.network.ipv4-pod-cidr: 100.64.2.0/24
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 02 Nov 2021 09:34:42 -0300
Taints:             especial=valor1:NoExecute
                    especial=valor1:NoSchedule

#podemos então remover o Taints de menos prioridade
kubectl taint nodes scw-k8s-angry-lovelace-default-104ea93176284d6 especial=valor1:NoSchedule-

#conferindo remoção
kubectl describe nodes scw-k8s-angry-lovelace-default-104ea93176284d6

Name:               scw-k8s-angry-lovelace-default-104ea93176284d6
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=DEV1-M
                    beta.kubernetes.io/os=linux
                    failure-domain.beta.kubernetes.io/region=fr-par
                    failure-domain.beta.kubernetes.io/zone=fr-par-1
                    k8s.scaleway.com/kapsule=39713063-f994-4dd6-8b91-f0d3fec3607a
                    k8s.scaleway.com/managed=true
                    k8s.scaleway.com/node=104ea931-7628-4d64-b901-85d3c55d499e
                    k8s.scaleway.com/pool=63f48a05-1ab1-49e1-916e-36aab1c39b88
                    k8s.scaleway.com/pool-name=default
                    k8s.scaleway.com/runtime=containerd
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=scw-k8s-angry-lovelace-default-104ea93176284d6
                    kubernetes.io/os=linux
                    node.kubernetes.io/instance-type=DEV1-M
                    topology.csi.scaleway.com/zone=fr-par-1
                    topology.kubernetes.io/region=fr-par
                    topology.kubernetes.io/zone=fr-par-1
Annotations:        csi.volume.kubernetes.io/nodeid: {"csi.scaleway.com":"fr-par-1/8e076646-d3f6-4531-915e-cdfd8b04dd28"}
                    io.cilium.network.ipv4-cilium-host: 100.64.2.130
                    io.cilium.network.ipv4-health-ip: 100.64.2.19
                    io.cilium.network.ipv4-pod-cidr: 100.64.2.0/24
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 02 Nov 2021 09:34:42 -0300
Taints:             especial=valor1:NoExecute

#retirando taint NoExecute para testar o próximo
kubectl taint nodes scw-k8s-angry-lovelace-default-104ea93176284d6 especial=valor1:NoExecute-

#verificando deployment executado
kubectl get pods -o wide

NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-7796b7557-589ph   1/1     Running   0          25m   100.64.0.218   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-7gbsq   1/1     Running   0          17m   100.64.2.20    scw-k8s-angry-lovelace-default-104ea93176284d6   <none>           <none>
nginx-deployment-7796b7557-944wt   1/1     Running   0          25m   100.64.1.186   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-bkv6r   1/1     Running   0          25m   100.64.0.91    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-c6jfp   1/1     Running   0          25m   100.64.0.105   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lm4nf   1/1     Running   0          25m   100.64.0.44    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lv5b7   1/1     Running   0          25m   100.64.1.30    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-t7gqg   1/1     Running   0          25m   100.64.1.79    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-xbzb7   1/1     Running   0          25m   100.64.1.219   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-z6lqx   1/1     Running   0          25m   100.64.1.165   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>

#diminuindo o número de réplicas
kubectl scale deployment nginx-deployment --replicas=8

NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-7796b7557-589ph   1/1     Running   0          26m   100.64.0.218   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-7gbsq   1/1     Running   0          19m   100.64.2.20    scw-k8s-angry-lovelace-default-104ea93176284d6   <none>           <none>
nginx-deployment-7796b7557-944wt   1/1     Running   0          26m   100.64.1.186   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-bkv6r   1/1     Running   0          26m   100.64.0.91    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-c6jfp   1/1     Running   0          26m   100.64.0.105   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lm4nf   1/1     Running   0          26m   100.64.0.44    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lv5b7   1/1     Running   0          26m   100.64.1.30    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-t7gqg   1/1     Running   0          26m   100.64.1.79    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>


#Adicionando Taints de PreferNoSchedule para que seja evitado a utilização do nó (scw-k8s-angry-lovelace-default-104ea93176284d6) quando possível
kubectl taint nodes scw-k8s-angry-lovelace-default-104ea93176284d6 especial=valor1:PreferNoSchedule

#verificando deployment executado
kubectl get pods -o wide

NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-7796b7557-589ph   1/1     Running   0          29m   100.64.0.218   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-7gbsq   1/1     Running   0          22m   100.64.2.20    scw-k8s-angry-lovelace-default-104ea93176284d6   <none>           <none>
nginx-deployment-7796b7557-944wt   1/1     Running   0          29m   100.64.1.186   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-bkv6r   1/1     Running   0          29m   100.64.0.91    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-c6jfp   1/1     Running   0          29m   100.64.0.105   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lm4nf   1/1     Running   0          29m   100.64.0.44    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lv5b7   1/1     Running   0          29m   100.64.1.30    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-t7gqg   1/1     Running   0          29m   100.64.1.79    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>

#como a configuração de Taints só afeta para os nóvos agendamentos, ainad terá pod rodando no nó, irei remover o deploy e aplicar novamente
kubectl delete -f ./taint_e_tolerations/deployment.yaml
kubectl apply -f ./taint_e_tolerations/deployment.yaml
kubectl scale deployment nginx-deployment --replicas=6

#verificando deployment executado
kubectl get pods -o wide

NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-7796b7557-6gkhd   1/1     Running   0          16s   100.64.1.237   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-6tkq9   1/1     Running   0          11s   100.64.0.200   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-hxl66   1/1     Running   0          11s   100.64.0.103   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-qvg98   1/1     Running   0          11s   100.64.1.50    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-rkfrs   1/1     Running   0          11s   100.64.0.228   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-xzrns   1/1     Running   0          11s   100.64.1.11    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>

#diminuindo a quantidade de réplicas deixando somente 6, foi possível alocar preferencialmente os pods nos dois nós do cluster evitando o nó scw-k8s-angry-lovelace-default-104ea93176284d6 onde foi configurado o PreferNoSchedule

#iniciando alteração para testar o tolerations, aplicando novamente o Taints de NoExecute
kubectl taint nodes scw-k8s-angry-lovelace-default-104ea93176284d6 especial=valor1:NoExecute

#verificando deployment em execução
kubectl get pods -o wide

NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-7796b7557-6gkhd   1/1     Running   0          22m   100.64.1.237   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-6tkq9   1/1     Running   0          22m   100.64.0.200   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-hxl66   1/1     Running   0          22m   100.64.0.103   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-qvg98   1/1     Running   0          22m   100.64.1.50    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-rkfrs   1/1     Running   0          22m   100.64.0.228   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-xzrns   1/1     Running   0          22m   100.64.1.11    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>

#aplicando as configurações de tolerations no pod para que permita execução no nó (scw-k8s-angry-lovelace-default-104ea93176284d6) onde foi aplicado o NoExecute
kubectl apply -f ./taint_e_tolerations/deployment.yaml

#verificando redistribuição do deployment depois da configuração do tolerations no pod
kubectl get pods -o wide

NAME                                READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES

nginx-deployment-5956f54fcc-8mzf6   1/1     Running   0          58s   100.64.0.165   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-5956f54fcc-brp92   1/1     Running   0          56s   100.64.1.70    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-5956f54fcc-h2xxx   1/1     Running   0          53s   100.64.2.175   scw-k8s-angry-lovelace-default-104ea93176284d6   <none>           <none>
nginx-deployment-5956f54fcc-l4vd8   1/1     Running   0          54s   100.64.0.213   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-5956f54fcc-nxsfq   1/1     Running   0          53s   100.64.1.149   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-5956f54fcc-qvgbj   1/1     Running   0          58s   100.64.1.10    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>

#nova redistribuição feita, e agora o pod (nginx-deployment-5956f54fcc-h2xxx) esta rodando no nó (scw-k8s-angry-lovelace-default-104ea93176284d6) que tem o Taints NoExecute configurado


#verificando os novos nós do cluster
kubectl get nodes

NAME                                             STATUS   ROLES    AGE   VERSION
scw-k8s-eager-haibt-default-99f1220659694ac6bb   Ready    <none>   16m   v1.22.3
scw-k8s-eager-haibt-default-9ed6f6cbef17408391   Ready    <none>   15m   v1.22.3
scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   Ready    <none>   15m   v1.22.3


#Limpando Taints do nó e aplicando configuração para Taints com tolerationSeconds de 60 segundos
kubectl taint nodes scw-k8s-eager-haibt-default-99f1220659694ac6bb especial=valor1:NoExecute-
kubectl taint nodes scw-k8s-eager-haibt-default-99f1220659694ac6bb especial=valor1:PreferNoSchedule-
kubectl apply -f ./taint_e_tolerations/deployment.yaml

#verificando pods vs nós de execução
kubectl get pods -o wide

NAME                               READY   STATUS    RESTARTS   AGE     IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-856cb49f4-cm9kp   1/1     Running   0          8m58s   100.64.0.89    scw-k8s-eager-haibt-default-99f1220659694ac6bb   <none>           <none>
nginx-deployment-856cb49f4-d5r7d   1/1     Running   0          8m58s   100.64.2.70    scw-k8s-eager-haibt-default-9ed6f6cbef17408391   <none>           <none>
nginx-deployment-856cb49f4-fj54q   1/1     Running   0          8m58s   100.64.1.67    scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-ggpjf   1/1     Running   0          8m58s   100.64.1.205   scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-vmdfr   1/1     Running   0          8m58s   100.64.2.29    scw-k8s-eager-haibt-default-9ed6f6cbef17408391   <none>           <none>
nginx-deployment-856cb49f4-w5dtf   1/1     Running   0          8m58s   100.64.0.142   scw-k8s-eager-haibt-default-99f1220659694ac6bb   <none>           <none>

#adicionando novamente o Taints NoExecute para demonstrar que terá uma tolerância de 60 segundos para os pods rodando neste nó ser reagendado para outro nó.
kubectl taint nodes scw-k8s-eager-haibt-default-99f1220659694ac6bb especial=valor1:NoExecute

#verificando duração para novo agendamento
kubectl get pods -o wide

#Iniciando o reagendamento
NAME                               READY   STATUS              RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-856cb49f4-4r54k   0/1     ContainerCreating   0          1s    <none>         scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-cm9kp   0/1     Terminating         0          11m   100.64.0.89    scw-k8s-eager-haibt-default-99f1220659694ac6bb   <none>           <none>
nginx-deployment-856cb49f4-d5r7d   1/1     Running             0          11m   100.64.2.70    scw-k8s-eager-haibt-default-9ed6f6cbef17408391   <none>           <none>
nginx-deployment-856cb49f4-fj54q   1/1     Running             0          11m   100.64.1.67    scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-ggpjf   1/1     Running             0          11m   100.64.1.205   scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-svbxg   0/1     ContainerCreating   0          1s    <none>         scw-k8s-eager-haibt-default-99f1220659694ac6bb   <none>           <none>
nginx-deployment-856cb49f4-vmdfr   1/1     Running             0          11m   100.64.2.29    scw-k8s-eager-haibt-default-9ed6f6cbef17408391   <none>           <none>
nginx-deployment-856cb49f4-w5dtf   1/1     Terminating         0          11m   100.64.0.142   scw-k8s-eager-haibt-default-99f1220659694ac6bb   <none>           <none>

#Reagendamento concluído, ainda existe 1 pod (nginx-deployment-856cb49f4-svbxg) rodando no nó, pois não existe taint para não agendamento 
# (NoSchedule), então foi agendado outro no mesmo nó, porém este terá novamente um período de 60 segundos para ser reagendado.
NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-856cb49f4-4r54k   1/1     Running   0          44s   100.64.1.60    scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-d5r7d   1/1     Running   0          12m   100.64.2.70    scw-k8s-eager-haibt-default-9ed6f6cbef17408391   <none>           <none>
nginx-deployment-856cb49f4-fj54q   1/1     Running   0          12m   100.64.1.67    scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-ggpjf   1/1     Running   0          12m   100.64.1.205   scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-svbxg   1/1     Running   0          44s   100.64.0.104   scw-k8s-eager-haibt-default-99f1220659694ac6bb   <none>           <none>
nginx-deployment-856cb49f4-vmdfr   1/1     Running   0          12m   100.64.2.29    scw-k8s-eager-haibt-default-9ed6f6cbef17408391   <none>           <none>

#Olhando novamente depois dos 60 segundos, perceba que o pod (nginx-deployment-856cb49f4-svbxg) foi substituído por (nginx-deployment-856cb49f4-5fs2f) ainda no mesmo nó (scw-k8s-eager-haibt-default-99f1220659694ac6bb)
NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-856cb49f4-4r54k   1/1     Running   0          6m    100.64.1.60    scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-5fs2f   1/1     Running   0          60s   100.64.0.192   scw-k8s-eager-haibt-default-99f1220659694ac6bb   <none>           <none>
nginx-deployment-856cb49f4-d5r7d   1/1     Running   0          17m   100.64.2.70    scw-k8s-eager-haibt-default-9ed6f6cbef17408391   <none>           <none>
nginx-deployment-856cb49f4-fj54q   1/1     Running   0          17m   100.64.1.67    scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-ggpjf   1/1     Running   0          17m   100.64.1.205   scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-vmdfr   1/1     Running   0          17m   100.64.2.29    scw-k8s-eager-haibt-default-9ed6f6cbef17408391   <none>           <none>

# pod (nginx-deployment-856cb49f4-5fs2f) trocado por (nginx-deployment-856cb49f4-pv4zw) ainda no mesmo nó
NAME                               READY   STATUS    RESTARTS   AGE    IP             NODE                                             NOMINATED NODE   READINESS GATES

nginx-deployment-856cb49f4-4r54k   1/1     Running   0          9m8s   100.64.1.60    scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-d5r7d   1/1     Running   0          21m    100.64.2.70    scw-k8s-eager-haibt-default-9ed6f6cbef17408391   <none>           <none>
nginx-deployment-856cb49f4-fj54q   1/1     Running   0          21m    100.64.1.67    scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-ggpjf   1/1     Running   0          21m    100.64.1.205   scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-pv4zw   1/1     Running   0          7s     100.64.0.71    scw-k8s-eager-haibt-default-99f1220659694ac6bb   <none>           <none>
nginx-deployment-856cb49f4-vmdfr   1/1     Running   0          21m    100.64.2.29    scw-k8s-eager-haibt-default-9ed6f6cbef17408391   <none>           <none>

#para que não seja mais possível cair agendamento no nó (scw-k8s-eager-haibt-default-99f1220659694ac6bb), basta adicionarmos um novo Taints de NoSchedule
kubectl taint nodes scw-k8s-eager-haibt-default-99f1220659694ac6bb especial=valor1:NoSchedule

#verificando antes da regra ser efetivamente respeitada
kubectl get pods -o wide

#pod (nginx-deployment-856cb49f4-n2482) ainda rodando no nó (scw-k8s-eager-haibt-default-99f1220659694ac6bb)
NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-856cb49f4-4r54k   1/1     Running   0          12m   100.64.1.60    scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-d5r7d   1/1     Running   0          24m   100.64.2.70    scw-k8s-eager-haibt-default-9ed6f6cbef17408391   <none>           <none>
nginx-deployment-856cb49f4-fj54q   1/1     Running   0          24m   100.64.1.67    scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-ggpjf   1/1     Running   0          24m   100.64.1.205   scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-n2482   1/1     Running   0          15s   100.64.0.157   scw-k8s-eager-haibt-default-99f1220659694ac6bb   <none>           <none>
nginx-deployment-856cb49f4-vmdfr   1/1     Running   0          24m   100.64.2.29    scw-k8s-eager-haibt-default-9ed6f6cbef17408391   <none>           <none>

#passado 60 segundos, não deverá mais ter agendamento para o nó (scw-k8s-eager-haibt-default-99f1220659694ac6bb)

NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-856cb49f4-4r54k   1/1     Running   0          13m   100.64.1.60    scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-d5r7d   1/1     Running   0          25m   100.64.2.70    scw-k8s-eager-haibt-default-9ed6f6cbef17408391   <none>           <none>
nginx-deployment-856cb49f4-fj54q   1/1     Running   0          25m   100.64.1.67    scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-ggpjf   1/1     Running   0          25m   100.64.1.205   scw-k8s-eager-haibt-default-c59e2a1f14744e5b86   <none>           <none>
nginx-deployment-856cb49f4-sv52n   1/1     Running   0          38s   100.64.2.71    scw-k8s-eager-haibt-default-9ed6f6cbef17408391   <none>           <none>
nginx-deployment-856cb49f4-vmdfr   1/1     Running   0          25m   100.64.2.29    scw-k8s-eager-haibt-default-9ed6f6cbef17408391   <none>           <none>