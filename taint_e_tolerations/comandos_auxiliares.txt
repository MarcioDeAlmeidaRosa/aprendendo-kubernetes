#aplicando deployment padrão para testar taint e tolerations
kubectl apply -f ./taint_e_tolerations/deployment.yaml

#verificando execução do pod
kubectl get pods -o wide

NAME                               READY   STATUS    RESTARTS   AGE   IP            NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-7796b7557-7q855   1/1     Running   0          47s   100.64.2.52   scw-k8s-angry-lovelace-default-104ea93176284d6   <none>           <none>

#verificando nós do clustes
kubectl get nodes

NAME                                             STATUS   ROLES    AGE    VERSION
scw-k8s-angry-lovelace-default-104ea93176284d6   Ready    <none>   156m   v1.22.3
scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   Ready    <none>   156m   v1.22.3
scw-k8s-angry-lovelace-default-fe175434bbd547e   Ready    <none>   156m   v1.22.3

#verificando Taints atribuídos ao nó, deve estar com o valor <none>:
kubectl describe node scw-k8s-angry-lovelace-default-104ea93176284d6

Name:               scw-k8s-angry-lovelace-default-104ea93176284d6
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=DEV1-M
                    beta.kubernetes.io/os=linux
                    failure-domain.beta.kubernetes.io/region=fr-par
                    failure-domain.beta.kubernetes.io/zone=fr-par-1
                    k8s.scaleway.com/kapsule=39713063-f994-4dd6-8b91-f0d3fec3607a
                    k8s.scaleway.com/managed=true
                    k8s.scaleway.com/node=104ea931-7628-4d64-b901-85d3c55d499e
                    k8s.scaleway.com/pool=63f48a05-1ab1-49e1-916e-36aab1c39b88
                    k8s.scaleway.com/pool-name=default
                    k8s.scaleway.com/runtime=containerd
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=scw-k8s-angry-lovelace-default-104ea93176284d6
                    kubernetes.io/os=linux
                    node.kubernetes.io/instance-type=DEV1-M
                    topology.csi.scaleway.com/zone=fr-par-1
                    topology.kubernetes.io/region=fr-par
                    topology.kubernetes.io/zone=fr-par-1
Annotations:        csi.volume.kubernetes.io/nodeid: {"csi.scaleway.com":"fr-par-1/8e076646-d3f6-4531-915e-cdfd8b04dd28"}
                    io.cilium.network.ipv4-cilium-host: 100.64.2.130
                    io.cilium.network.ipv4-health-ip: 100.64.2.19
                    io.cilium.network.ipv4-pod-cidr: 100.64.2.0/24
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 02 Nov 2021 09:34:42 -0300
Taints:             <none>

#aplicando Taints de NoSchedule para que não permita novos agendamento de pods novos no nó scw-k8s-angry-lovelace-default-104ea93176284d6
kubectl taint nodes scw-k8s-angry-lovelace-default-104ea93176284d6 especial=valor1:NoSchedule

#verificando nós do clustes
kubectl describe nodes scw-k8s-angry-lovelace-default-104ea93176284d6

Name:               scw-k8s-angry-lovelace-default-104ea93176284d6
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=DEV1-M
                    beta.kubernetes.io/os=linux
                    failure-domain.beta.kubernetes.io/region=fr-par
                    failure-domain.beta.kubernetes.io/zone=fr-par-1
                    k8s.scaleway.com/kapsule=39713063-f994-4dd6-8b91-f0d3fec3607a
                    k8s.scaleway.com/managed=true
                    k8s.scaleway.com/node=104ea931-7628-4d64-b901-85d3c55d499e
                    k8s.scaleway.com/pool=63f48a05-1ab1-49e1-916e-36aab1c39b88
                    k8s.scaleway.com/pool-name=default
                    k8s.scaleway.com/runtime=containerd
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=scw-k8s-angry-lovelace-default-104ea93176284d6
                    kubernetes.io/os=linux
                    node.kubernetes.io/instance-type=DEV1-M
                    topology.csi.scaleway.com/zone=fr-par-1
                    topology.kubernetes.io/region=fr-par
                    topology.kubernetes.io/zone=fr-par-1
Annotations:        csi.volume.kubernetes.io/nodeid: {"csi.scaleway.com":"fr-par-1/8e076646-d3f6-4531-915e-cdfd8b04dd28"}
                    io.cilium.network.ipv4-cilium-host: 100.64.2.130
                    io.cilium.network.ipv4-health-ip: 100.64.2.19
                    io.cilium.network.ipv4-pod-cidr: 100.64.2.0/24
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 02 Nov 2021 09:34:42 -0300
Taints:             especial=valor1:NoSchedule
Unschedulable:      false


#escalando o deployment para verificar os nós que eles serão feitos deploy
kubectl scale deployment nginx-deployment --replicas=10

#verificando deployment executado
kubectl get pods -o wide

NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-7796b7557-589ph   1/1     Running   0          39s   100.64.0.218   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-7q855   1/1     Running   0          13m   100.64.2.52    scw-k8s-angry-lovelace-default-104ea93176284d6   <none>           <none>
nginx-deployment-7796b7557-944wt   1/1     Running   0          39s   100.64.1.186   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-bkv6r   1/1     Running   0          39s   100.64.0.91    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-c6jfp   1/1     Running   0          39s   100.64.0.105   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lm4nf   1/1     Running   0          40s   100.64.0.44    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lv5b7   1/1     Running   0          39s   100.64.1.30    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-t7gqg   1/1     Running   0          39s   100.64.1.79    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-xbzb7   1/1     Running   0          40s   100.64.1.219   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-z6lqx   1/1     Running   0          40s   100.64.1.165   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>

#aplicando Taints de NoExecute para que não permita mais execução de pod nenhum no nó scw-k8s-angry-lovelace-default-104ea93176284d6
kubectl taint nodes scw-k8s-angry-lovelace-default-104ea93176284d6 especial=valor1:NoExecute

#verificando deployment executado
kubectl get pods -o wide

NAME                               READY   STATUS    RESTARTS   AGE     IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-7796b7557-589ph   1/1     Running   0          8m38s   100.64.0.218   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-7gbsq   0/1     Pending   0          61s     <none>         <none>                                           <none>           <none>
nginx-deployment-7796b7557-944wt   1/1     Running   0          8m38s   100.64.1.186   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-bkv6r   1/1     Running   0          8m38s   100.64.0.91    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-c6jfp   1/1     Running   0          8m38s   100.64.0.105   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lm4nf   1/1     Running   0          8m39s   100.64.0.44    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lv5b7   1/1     Running   0          8m38s   100.64.1.30    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-t7gqg   1/1     Running   0          8m38s   100.64.1.79    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-xbzb7   1/1     Running   0          8m39s   100.64.1.219   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-z6lqx   1/1     Running   0          8m39s   100.64.1.165   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>

#ficou pendente por que não tem mais recurso de máquina para usar, mas foi removido o pod de execução (nginx-deployment-7796b7557-7q855)

#os Taints são "acumulados" ao ser adicionado no nó, podemos ver isso fazendo um describe no nó.
kubectl describe nodes scw-k8s-angry-lovelace-default-104ea93176284d6

Name:               scw-k8s-angry-lovelace-default-104ea93176284d6
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=DEV1-M
                    beta.kubernetes.io/os=linux
                    failure-domain.beta.kubernetes.io/region=fr-par
                    failure-domain.beta.kubernetes.io/zone=fr-par-1
                    k8s.scaleway.com/kapsule=39713063-f994-4dd6-8b91-f0d3fec3607a
                    k8s.scaleway.com/managed=true
                    k8s.scaleway.com/node=104ea931-7628-4d64-b901-85d3c55d499e
                    k8s.scaleway.com/pool=63f48a05-1ab1-49e1-916e-36aab1c39b88
                    k8s.scaleway.com/pool-name=default
                    k8s.scaleway.com/runtime=containerd
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=scw-k8s-angry-lovelace-default-104ea93176284d6
                    kubernetes.io/os=linux
                    node.kubernetes.io/instance-type=DEV1-M
                    topology.csi.scaleway.com/zone=fr-par-1
                    topology.kubernetes.io/region=fr-par
                    topology.kubernetes.io/zone=fr-par-1
Annotations:        csi.volume.kubernetes.io/nodeid: {"csi.scaleway.com":"fr-par-1/8e076646-d3f6-4531-915e-cdfd8b04dd28"}
                    io.cilium.network.ipv4-cilium-host: 100.64.2.130
                    io.cilium.network.ipv4-health-ip: 100.64.2.19
                    io.cilium.network.ipv4-pod-cidr: 100.64.2.0/24
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 02 Nov 2021 09:34:42 -0300
Taints:             especial=valor1:NoExecute
                    especial=valor1:NoSchedule

#podemos então remover o Taints de menos prioridade
kubectl taint nodes scw-k8s-angry-lovelace-default-104ea93176284d6 especial=valor1:NoSchedule-

#conferindo remoção
kubectl describe nodes scw-k8s-angry-lovelace-default-104ea93176284d6

Name:               scw-k8s-angry-lovelace-default-104ea93176284d6
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=DEV1-M
                    beta.kubernetes.io/os=linux
                    failure-domain.beta.kubernetes.io/region=fr-par
                    failure-domain.beta.kubernetes.io/zone=fr-par-1
                    k8s.scaleway.com/kapsule=39713063-f994-4dd6-8b91-f0d3fec3607a
                    k8s.scaleway.com/managed=true
                    k8s.scaleway.com/node=104ea931-7628-4d64-b901-85d3c55d499e
                    k8s.scaleway.com/pool=63f48a05-1ab1-49e1-916e-36aab1c39b88
                    k8s.scaleway.com/pool-name=default
                    k8s.scaleway.com/runtime=containerd
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=scw-k8s-angry-lovelace-default-104ea93176284d6
                    kubernetes.io/os=linux
                    node.kubernetes.io/instance-type=DEV1-M
                    topology.csi.scaleway.com/zone=fr-par-1
                    topology.kubernetes.io/region=fr-par
                    topology.kubernetes.io/zone=fr-par-1
Annotations:        csi.volume.kubernetes.io/nodeid: {"csi.scaleway.com":"fr-par-1/8e076646-d3f6-4531-915e-cdfd8b04dd28"}
                    io.cilium.network.ipv4-cilium-host: 100.64.2.130
                    io.cilium.network.ipv4-health-ip: 100.64.2.19
                    io.cilium.network.ipv4-pod-cidr: 100.64.2.0/24
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 02 Nov 2021 09:34:42 -0300
Taints:             especial=valor1:NoExecute

#retirando taint NoExecute para testar o próximo
kubectl taint nodes scw-k8s-angry-lovelace-default-104ea93176284d6 especial=valor1:NoExecute-

#verificando deployment executado
kubectl get pods -o wide

NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-7796b7557-589ph   1/1     Running   0          25m   100.64.0.218   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-7gbsq   1/1     Running   0          17m   100.64.2.20    scw-k8s-angry-lovelace-default-104ea93176284d6   <none>           <none>
nginx-deployment-7796b7557-944wt   1/1     Running   0          25m   100.64.1.186   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-bkv6r   1/1     Running   0          25m   100.64.0.91    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-c6jfp   1/1     Running   0          25m   100.64.0.105   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lm4nf   1/1     Running   0          25m   100.64.0.44    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lv5b7   1/1     Running   0          25m   100.64.1.30    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-t7gqg   1/1     Running   0          25m   100.64.1.79    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-xbzb7   1/1     Running   0          25m   100.64.1.219   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-z6lqx   1/1     Running   0          25m   100.64.1.165   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>

#diminuindo o número de réplicas
kubectl scale deployment nginx-deployment --replicas=8

NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-7796b7557-589ph   1/1     Running   0          26m   100.64.0.218   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-7gbsq   1/1     Running   0          19m   100.64.2.20    scw-k8s-angry-lovelace-default-104ea93176284d6   <none>           <none>
nginx-deployment-7796b7557-944wt   1/1     Running   0          26m   100.64.1.186   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-bkv6r   1/1     Running   0          26m   100.64.0.91    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-c6jfp   1/1     Running   0          26m   100.64.0.105   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lm4nf   1/1     Running   0          26m   100.64.0.44    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lv5b7   1/1     Running   0          26m   100.64.1.30    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-t7gqg   1/1     Running   0          26m   100.64.1.79    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>


#Adicionando Taints de PreferNoSchedule para que seja evitado a utilização do nó (scw-k8s-angry-lovelace-default-104ea93176284d6) quando possível
kubectl taint nodes scw-k8s-angry-lovelace-default-104ea93176284d6 especial=valor1:PreferNoSchedule

#verificando deployment executado
kubectl get pods -o wide

NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-7796b7557-589ph   1/1     Running   0          29m   100.64.0.218   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-7gbsq   1/1     Running   0          22m   100.64.2.20    scw-k8s-angry-lovelace-default-104ea93176284d6   <none>           <none>
nginx-deployment-7796b7557-944wt   1/1     Running   0          29m   100.64.1.186   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-bkv6r   1/1     Running   0          29m   100.64.0.91    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-c6jfp   1/1     Running   0          29m   100.64.0.105   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lm4nf   1/1     Running   0          29m   100.64.0.44    scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-lv5b7   1/1     Running   0          29m   100.64.1.30    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-t7gqg   1/1     Running   0          29m   100.64.1.79    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>

#como a configuração de Taints só afeta para os nóvos agendamentos, ainad terá pod rodando no nó, irei remover o deploy e aplicar novamente
kubectl delete -f ./taint_e_tolerations/deployment.yaml
kubectl apply -f ./taint_e_tolerations/deployment.yaml
kubectl scale deployment nginx-deployment --replicas=6

#verificando deployment executado
kubectl get pods -o wide

NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-7796b7557-6gkhd   1/1     Running   0          16s   100.64.1.237   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-6tkq9   1/1     Running   0          11s   100.64.0.200   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-hxl66   1/1     Running   0          11s   100.64.0.103   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-qvg98   1/1     Running   0          11s   100.64.1.50    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-rkfrs   1/1     Running   0          11s   100.64.0.228   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-xzrns   1/1     Running   0          11s   100.64.1.11    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>

#diminuindo a quantidade de réplicas deixando somente 6, foi possível alocar preferencialmente os pods nos dois nós do cluster evitando o nó scw-k8s-angry-lovelace-default-104ea93176284d6 onde foi configurado o PreferNoSchedule

#iniciando alteração para testar o tolerations, aplicando novamente o Taints de NoExecute
kubectl taint nodes scw-k8s-angry-lovelace-default-104ea93176284d6 especial=valor1:NoExecute

#verificando deployment em execução
kubectl get pods -o wide

NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES
nginx-deployment-7796b7557-6gkhd   1/1     Running   0          22m   100.64.1.237   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-6tkq9   1/1     Running   0          22m   100.64.0.200   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-hxl66   1/1     Running   0          22m   100.64.0.103   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-qvg98   1/1     Running   0          22m   100.64.1.50    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-7796b7557-rkfrs   1/1     Running   0          22m   100.64.0.228   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-7796b7557-xzrns   1/1     Running   0          22m   100.64.1.11    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>

#aplicando as configurações de tolerations no pod para que permita execução no nó (scw-k8s-angry-lovelace-default-104ea93176284d6) onde foi aplicado o NoExecute
kubectl apply -f ./taint_e_tolerations/deployment.yaml

#verificando redistribuição do deployment depois da configuração do tolerations no pod
kubectl get pods -o wide

NAME                                READY   STATUS    RESTARTS   AGE   IP             NODE                                             NOMINATED NODE   READINESS GATES

nginx-deployment-5956f54fcc-8mzf6   1/1     Running   0          58s   100.64.0.165   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-5956f54fcc-brp92   1/1     Running   0          56s   100.64.1.70    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-5956f54fcc-h2xxx   1/1     Running   0          53s   100.64.2.175   scw-k8s-angry-lovelace-default-104ea93176284d6   <none>           <none>
nginx-deployment-5956f54fcc-l4vd8   1/1     Running   0          54s   100.64.0.213   scw-k8s-angry-lovelace-default-fe175434bbd547e   <none>           <none>
nginx-deployment-5956f54fcc-nxsfq   1/1     Running   0          53s   100.64.1.149   scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>
nginx-deployment-5956f54fcc-qvgbj   1/1     Running   0          58s   100.64.1.10    scw-k8s-angry-lovelace-default-4fd0f1d8c47946d   <none>           <none>

#nova redistribuição feita, e agora o pod (nginx-deployment-5956f54fcc-h2xxx) esta rodando no nó (scw-k8s-angry-lovelace-default-104ea93176284d6) que tem o Taints NoExecute configurado